{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-02T16:03:16-06:00"
    },
    {
      "path": "FinalProject.html",
      "title": "Final Project",
      "description": "New dataset\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nThe dataset used for this project come from Kaggle, and it shows how fuel prices correlate with Unemployment. In this project, I will try to predict that the higher the unemployment leads to higher gas prices.\r\nWe begin by looking at the descriptive analytics.The standard deviation shows that Fuel Prices are close to the norm at 0.43, while Unemployment is skewed by 1.88.\r\nThe machine learning model used is the K-nn (Nearest Neighbor) model. This model is an algorithim that can solve classification and regression problems.\r\n\r\n\r\nload(file=\"Features_data_set.rda\")\r\nlibrary(tidyverse)\r\nlibrary(caret)\r\nFeatures_data_set=na.omit(Features_data_set)\r\n\r\n\r\n\r\n\r\n\r\nknitr::kable(psych::describe(Features_data_set))%>%\r\n  kableExtra::kable_styling(\"striped\")%>%\r\n  kableExtra::scroll_box(width = \"100%\",height=\"300px\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nvars\r\n\r\n\r\nn\r\n\r\n\r\nmean\r\n\r\n\r\nsd\r\n\r\n\r\nmedian\r\n\r\n\r\ntrimmed\r\n\r\n\r\nmad\r\n\r\n\r\nmin\r\n\r\n\r\nmax\r\n\r\n\r\nrange\r\n\r\n\r\nskew\r\n\r\n\r\nkurtosis\r\n\r\n\r\nse\r\n\r\n\r\nStore\r\n\r\n\r\n1\r\n\r\n\r\n2069\r\n\r\n\r\n20.386660\r\n\r\n\r\n1.207617e+01\r\n\r\n\r\n20.0000\r\n\r\n\r\n19.974653\r\n\r\n\r\n13.3434000\r\n\r\n\r\n1.0000\r\n\r\n\r\n45.0000\r\n\r\n\r\n44.00000\r\n\r\n\r\n0.2291798\r\n\r\n\r\n-0.9199640\r\n\r\n\r\n0.2654906\r\n\r\n\r\nDate*\r\n\r\n\r\n2\r\n\r\n\r\n2069\r\n\r\n\r\n35.101982\r\n\r\n\r\n2.076426e+01\r\n\r\n\r\n34.0000\r\n\r\n\r\n35.008449\r\n\r\n\r\n26.6868000\r\n\r\n\r\n1.0000\r\n\r\n\r\n70.0000\r\n\r\n\r\n69.00000\r\n\r\n\r\n0.0545191\r\n\r\n\r\n-1.2317835\r\n\r\n\r\n0.4564952\r\n\r\n\r\nTemperature\r\n\r\n\r\n3\r\n\r\n\r\n2069\r\n\r\n\r\n52.516979\r\n\r\n\r\n1.848305e+01\r\n\r\n\r\n51.4200\r\n\r\n\r\n52.406089\r\n\r\n\r\n21.0084420\r\n\r\n\r\n-7.2900\r\n\r\n\r\n95.9100\r\n\r\n\r\n103.20000\r\n\r\n\r\n0.0301572\r\n\r\n\r\n-0.6253938\r\n\r\n\r\n0.4063436\r\n\r\n\r\nFuel_Price\r\n\r\n\r\n4\r\n\r\n\r\n2069\r\n\r\n\r\n3.600298\r\n\r\n\r\n2.793347e-01\r\n\r\n\r\n3.6100\r\n\r\n\r\n3.603051\r\n\r\n\r\n0.2905896\r\n\r\n\r\n2.8720\r\n\r\n\r\n4.3010\r\n\r\n\r\n1.42900\r\n\r\n\r\n-0.1089610\r\n\r\n\r\n-0.4844894\r\n\r\n\r\n0.0061411\r\n\r\n\r\nMarkDown1\r\n\r\n\r\n5\r\n\r\n\r\n2069\r\n\r\n\r\n9717.665297\r\n\r\n\r\n1.137764e+04\r\n\r\n\r\n6397.7200\r\n\r\n\r\n7472.803362\r\n\r\n\r\n5132.3312460\r\n\r\n\r\n-563.9000\r\n\r\n\r\n103184.9800\r\n\r\n\r\n103748.88000\r\n\r\n\r\n3.4678921\r\n\r\n\r\n15.8541281\r\n\r\n\r\n250.1334788\r\n\r\n\r\nMarkDown2\r\n\r\n\r\n6\r\n\r\n\r\n2069\r\n\r\n\r\n4426.859778\r\n\r\n\r\n1.015466e+04\r\n\r\n\r\n404.6100\r\n\r\n\r\n1881.139300\r\n\r\n\r\n590.9791860\r\n\r\n\r\n-265.7600\r\n\r\n\r\n104519.5400\r\n\r\n\r\n104785.30000\r\n\r\n\r\n4.2007117\r\n\r\n\r\n23.0579620\r\n\r\n\r\n223.2467055\r\n\r\n\r\nMarkDown3\r\n\r\n\r\n7\r\n\r\n\r\n2069\r\n\r\n\r\n2247.535911\r\n\r\n\r\n1.330599e+04\r\n\r\n\r\n34.1000\r\n\r\n\r\n76.566494\r\n\r\n\r\n47.5914600\r\n\r\n\r\n-179.2600\r\n\r\n\r\n149483.3100\r\n\r\n\r\n149662.57000\r\n\r\n\r\n7.1749577\r\n\r\n\r\n55.1897612\r\n\r\n\r\n292.5275704\r\n\r\n\r\nMarkDown4\r\n\r\n\r\n8\r\n\r\n\r\n2069\r\n\r\n\r\n4213.162687\r\n\r\n\r\n8.359766e+03\r\n\r\n\r\n1401.2300\r\n\r\n\r\n2218.794997\r\n\r\n\r\n1837.7123520\r\n\r\n\r\n0.4600\r\n\r\n\r\n67474.8500\r\n\r\n\r\n67474.39000\r\n\r\n\r\n3.9881817\r\n\r\n\r\n18.3368551\r\n\r\n\r\n183.7866133\r\n\r\n\r\nMarkDown5\r\n\r\n\r\n9\r\n\r\n\r\n2069\r\n\r\n\r\n5056.916061\r\n\r\n\r\n1.780717e+04\r\n\r\n\r\n3470.4900\r\n\r\n\r\n3817.556240\r\n\r\n\r\n2491.0348680\r\n\r\n\r\n-185.1700\r\n\r\n\r\n771448.1000\r\n\r\n\r\n771633.27000\r\n\r\n\r\n38.8026276\r\n\r\n\r\n1656.5462412\r\n\r\n\r\n391.4845448\r\n\r\n\r\nCPI\r\n\r\n\r\n10\r\n\r\n\r\n2069\r\n\r\n\r\n175.587370\r\n\r\n\r\n3.996391e+01\r\n\r\n\r\n189.7076\r\n\r\n\r\n174.926293\r\n\r\n\r\n56.7786812\r\n\r\n\r\n129.8167\r\n\r\n\r\n228.9765\r\n\r\n\r\n99.15975\r\n\r\n\r\n0.0757006\r\n\r\n\r\n-1.8312551\r\n\r\n\r\n0.8785931\r\n\r\n\r\nUnemployment\r\n\r\n\r\n11\r\n\r\n\r\n2069\r\n\r\n\r\n7.252611\r\n\r\n\r\n1.684774e+00\r\n\r\n\r\n7.1910\r\n\r\n\r\n7.207248\r\n\r\n\r\n1.5700734\r\n\r\n\r\n3.6840\r\n\r\n\r\n12.8900\r\n\r\n\r\n9.20600\r\n\r\n\r\n0.4678311\r\n\r\n\r\n0.8763615\r\n\r\n\r\n0.0370392\r\n\r\n\r\nIsHoliday\r\n\r\n\r\n12\r\n\r\n\r\n2069\r\n\r\n\r\nNaN\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNaN\r\n\r\n\r\nNA\r\n\r\n\r\nInf\r\n\r\n\r\n-Inf\r\n\r\n\r\n-Inf\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n\r\n\r\n\r\nggplot(data=Features_data_set,mapping = aes(x=Fuel_Price,y=Unemployment,color=Store))+geom_point(alpha=0.5)\r\n\r\n\r\n\r\n\r\n\r\n\r\n#this creates an index of rows in include in the training\r\n#literally lists the rows to keep\r\ntrainIndex <- createDataPartition(Features_data_set$Fuel_Price, p = .6, list = FALSE, times = 1)\r\n\r\n#rows to keep\r\nknitr::kable(trainIndex)%>%\r\n  kableExtra::kable_styling(\"striped\")%>%\r\n  kableExtra::scroll_box(width = \"100%\",height=\"200px\")\r\n\r\n\r\n\r\n\r\nResample1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n5\r\n\r\n\r\n6\r\n\r\n\r\n7\r\n\r\n\r\n9\r\n\r\n\r\n12\r\n\r\n\r\n14\r\n\r\n\r\n16\r\n\r\n\r\n17\r\n\r\n\r\n19\r\n\r\n\r\n20\r\n\r\n\r\n26\r\n\r\n\r\n27\r\n\r\n\r\n29\r\n\r\n\r\n30\r\n\r\n\r\n31\r\n\r\n\r\n32\r\n\r\n\r\n33\r\n\r\n\r\n36\r\n\r\n\r\n37\r\n\r\n\r\n41\r\n\r\n\r\n42\r\n\r\n\r\n43\r\n\r\n\r\n46\r\n\r\n\r\n48\r\n\r\n\r\n49\r\n\r\n\r\n51\r\n\r\n\r\n52\r\n\r\n\r\n53\r\n\r\n\r\n54\r\n\r\n\r\n55\r\n\r\n\r\n57\r\n\r\n\r\n58\r\n\r\n\r\n59\r\n\r\n\r\n60\r\n\r\n\r\n63\r\n\r\n\r\n64\r\n\r\n\r\n65\r\n\r\n\r\n66\r\n\r\n\r\n67\r\n\r\n\r\n69\r\n\r\n\r\n70\r\n\r\n\r\n71\r\n\r\n\r\n72\r\n\r\n\r\n73\r\n\r\n\r\n75\r\n\r\n\r\n78\r\n\r\n\r\n79\r\n\r\n\r\n82\r\n\r\n\r\n84\r\n\r\n\r\n85\r\n\r\n\r\n86\r\n\r\n\r\n91\r\n\r\n\r\n94\r\n\r\n\r\n97\r\n\r\n\r\n100\r\n\r\n\r\n102\r\n\r\n\r\n105\r\n\r\n\r\n107\r\n\r\n\r\n108\r\n\r\n\r\n109\r\n\r\n\r\n112\r\n\r\n\r\n114\r\n\r\n\r\n115\r\n\r\n\r\n117\r\n\r\n\r\n119\r\n\r\n\r\n120\r\n\r\n\r\n121\r\n\r\n\r\n122\r\n\r\n\r\n124\r\n\r\n\r\n125\r\n\r\n\r\n127\r\n\r\n\r\n128\r\n\r\n\r\n131\r\n\r\n\r\n132\r\n\r\n\r\n133\r\n\r\n\r\n135\r\n\r\n\r\n137\r\n\r\n\r\n140\r\n\r\n\r\n142\r\n\r\n\r\n144\r\n\r\n\r\n146\r\n\r\n\r\n147\r\n\r\n\r\n151\r\n\r\n\r\n152\r\n\r\n\r\n154\r\n\r\n\r\n156\r\n\r\n\r\n158\r\n\r\n\r\n159\r\n\r\n\r\n162\r\n\r\n\r\n163\r\n\r\n\r\n165\r\n\r\n\r\n167\r\n\r\n\r\n168\r\n\r\n\r\n173\r\n\r\n\r\n174\r\n\r\n\r\n175\r\n\r\n\r\n183\r\n\r\n\r\n184\r\n\r\n\r\n185\r\n\r\n\r\n187\r\n\r\n\r\n188\r\n\r\n\r\n189\r\n\r\n\r\n191\r\n\r\n\r\n192\r\n\r\n\r\n193\r\n\r\n\r\n194\r\n\r\n\r\n195\r\n\r\n\r\n197\r\n\r\n\r\n198\r\n\r\n\r\n199\r\n\r\n\r\n200\r\n\r\n\r\n201\r\n\r\n\r\n202\r\n\r\n\r\n203\r\n\r\n\r\n210\r\n\r\n\r\n211\r\n\r\n\r\n212\r\n\r\n\r\n215\r\n\r\n\r\n216\r\n\r\n\r\n217\r\n\r\n\r\n218\r\n\r\n\r\n219\r\n\r\n\r\n220\r\n\r\n\r\n221\r\n\r\n\r\n222\r\n\r\n\r\n223\r\n\r\n\r\n225\r\n\r\n\r\n228\r\n\r\n\r\n230\r\n\r\n\r\n235\r\n\r\n\r\n236\r\n\r\n\r\n237\r\n\r\n\r\n238\r\n\r\n\r\n239\r\n\r\n\r\n240\r\n\r\n\r\n241\r\n\r\n\r\n242\r\n\r\n\r\n243\r\n\r\n\r\n245\r\n\r\n\r\n248\r\n\r\n\r\n251\r\n\r\n\r\n252\r\n\r\n\r\n253\r\n\r\n\r\n255\r\n\r\n\r\n256\r\n\r\n\r\n257\r\n\r\n\r\n258\r\n\r\n\r\n260\r\n\r\n\r\n262\r\n\r\n\r\n263\r\n\r\n\r\n265\r\n\r\n\r\n266\r\n\r\n\r\n268\r\n\r\n\r\n269\r\n\r\n\r\n271\r\n\r\n\r\n273\r\n\r\n\r\n274\r\n\r\n\r\n275\r\n\r\n\r\n276\r\n\r\n\r\n277\r\n\r\n\r\n279\r\n\r\n\r\n280\r\n\r\n\r\n281\r\n\r\n\r\n283\r\n\r\n\r\n284\r\n\r\n\r\n285\r\n\r\n\r\n286\r\n\r\n\r\n287\r\n\r\n\r\n288\r\n\r\n\r\n289\r\n\r\n\r\n290\r\n\r\n\r\n292\r\n\r\n\r\n293\r\n\r\n\r\n294\r\n\r\n\r\n295\r\n\r\n\r\n296\r\n\r\n\r\n297\r\n\r\n\r\n299\r\n\r\n\r\n300\r\n\r\n\r\n301\r\n\r\n\r\n303\r\n\r\n\r\n304\r\n\r\n\r\n305\r\n\r\n\r\n306\r\n\r\n\r\n307\r\n\r\n\r\n308\r\n\r\n\r\n309\r\n\r\n\r\n311\r\n\r\n\r\n312\r\n\r\n\r\n315\r\n\r\n\r\n316\r\n\r\n\r\n317\r\n\r\n\r\n320\r\n\r\n\r\n321\r\n\r\n\r\n322\r\n\r\n\r\n326\r\n\r\n\r\n327\r\n\r\n\r\n329\r\n\r\n\r\n331\r\n\r\n\r\n334\r\n\r\n\r\n336\r\n\r\n\r\n337\r\n\r\n\r\n338\r\n\r\n\r\n339\r\n\r\n\r\n342\r\n\r\n\r\n344\r\n\r\n\r\n345\r\n\r\n\r\n348\r\n\r\n\r\n350\r\n\r\n\r\n351\r\n\r\n\r\n352\r\n\r\n\r\n353\r\n\r\n\r\n354\r\n\r\n\r\n356\r\n\r\n\r\n357\r\n\r\n\r\n359\r\n\r\n\r\n360\r\n\r\n\r\n361\r\n\r\n\r\n362\r\n\r\n\r\n363\r\n\r\n\r\n365\r\n\r\n\r\n366\r\n\r\n\r\n367\r\n\r\n\r\n370\r\n\r\n\r\n371\r\n\r\n\r\n375\r\n\r\n\r\n376\r\n\r\n\r\n378\r\n\r\n\r\n380\r\n\r\n\r\n381\r\n\r\n\r\n382\r\n\r\n\r\n383\r\n\r\n\r\n386\r\n\r\n\r\n387\r\n\r\n\r\n390\r\n\r\n\r\n391\r\n\r\n\r\n392\r\n\r\n\r\n393\r\n\r\n\r\n394\r\n\r\n\r\n395\r\n\r\n\r\n396\r\n\r\n\r\n398\r\n\r\n\r\n399\r\n\r\n\r\n400\r\n\r\n\r\n401\r\n\r\n\r\n402\r\n\r\n\r\n404\r\n\r\n\r\n405\r\n\r\n\r\n408\r\n\r\n\r\n415\r\n\r\n\r\n416\r\n\r\n\r\n417\r\n\r\n\r\n420\r\n\r\n\r\n422\r\n\r\n\r\n425\r\n\r\n\r\n426\r\n\r\n\r\n427\r\n\r\n\r\n428\r\n\r\n\r\n431\r\n\r\n\r\n432\r\n\r\n\r\n433\r\n\r\n\r\n434\r\n\r\n\r\n435\r\n\r\n\r\n437\r\n\r\n\r\n438\r\n\r\n\r\n441\r\n\r\n\r\n443\r\n\r\n\r\n444\r\n\r\n\r\n445\r\n\r\n\r\n446\r\n\r\n\r\n447\r\n\r\n\r\n450\r\n\r\n\r\n451\r\n\r\n\r\n453\r\n\r\n\r\n458\r\n\r\n\r\n459\r\n\r\n\r\n460\r\n\r\n\r\n461\r\n\r\n\r\n463\r\n\r\n\r\n464\r\n\r\n\r\n467\r\n\r\n\r\n468\r\n\r\n\r\n470\r\n\r\n\r\n472\r\n\r\n\r\n473\r\n\r\n\r\n476\r\n\r\n\r\n477\r\n\r\n\r\n478\r\n\r\n\r\n481\r\n\r\n\r\n482\r\n\r\n\r\n484\r\n\r\n\r\n485\r\n\r\n\r\n486\r\n\r\n\r\n487\r\n\r\n\r\n489\r\n\r\n\r\n490\r\n\r\n\r\n491\r\n\r\n\r\n495\r\n\r\n\r\n496\r\n\r\n\r\n497\r\n\r\n\r\n499\r\n\r\n\r\n500\r\n\r\n\r\n502\r\n\r\n\r\n503\r\n\r\n\r\n505\r\n\r\n\r\n506\r\n\r\n\r\n510\r\n\r\n\r\n511\r\n\r\n\r\n513\r\n\r\n\r\n514\r\n\r\n\r\n515\r\n\r\n\r\n517\r\n\r\n\r\n519\r\n\r\n\r\n520\r\n\r\n\r\n521\r\n\r\n\r\n523\r\n\r\n\r\n526\r\n\r\n\r\n527\r\n\r\n\r\n528\r\n\r\n\r\n532\r\n\r\n\r\n535\r\n\r\n\r\n537\r\n\r\n\r\n538\r\n\r\n\r\n540\r\n\r\n\r\n541\r\n\r\n\r\n542\r\n\r\n\r\n543\r\n\r\n\r\n544\r\n\r\n\r\n545\r\n\r\n\r\n546\r\n\r\n\r\n549\r\n\r\n\r\n551\r\n\r\n\r\n552\r\n\r\n\r\n553\r\n\r\n\r\n554\r\n\r\n\r\n555\r\n\r\n\r\n557\r\n\r\n\r\n558\r\n\r\n\r\n560\r\n\r\n\r\n561\r\n\r\n\r\n562\r\n\r\n\r\n563\r\n\r\n\r\n564\r\n\r\n\r\n569\r\n\r\n\r\n571\r\n\r\n\r\n573\r\n\r\n\r\n574\r\n\r\n\r\n575\r\n\r\n\r\n578\r\n\r\n\r\n582\r\n\r\n\r\n583\r\n\r\n\r\n584\r\n\r\n\r\n585\r\n\r\n\r\n586\r\n\r\n\r\n587\r\n\r\n\r\n590\r\n\r\n\r\n591\r\n\r\n\r\n592\r\n\r\n\r\n593\r\n\r\n\r\n594\r\n\r\n\r\n597\r\n\r\n\r\n598\r\n\r\n\r\n599\r\n\r\n\r\n600\r\n\r\n\r\n606\r\n\r\n\r\n607\r\n\r\n\r\n609\r\n\r\n\r\n611\r\n\r\n\r\n612\r\n\r\n\r\n613\r\n\r\n\r\n615\r\n\r\n\r\n617\r\n\r\n\r\n618\r\n\r\n\r\n621\r\n\r\n\r\n623\r\n\r\n\r\n624\r\n\r\n\r\n626\r\n\r\n\r\n627\r\n\r\n\r\n633\r\n\r\n\r\n634\r\n\r\n\r\n635\r\n\r\n\r\n636\r\n\r\n\r\n637\r\n\r\n\r\n638\r\n\r\n\r\n639\r\n\r\n\r\n642\r\n\r\n\r\n646\r\n\r\n\r\n647\r\n\r\n\r\n649\r\n\r\n\r\n651\r\n\r\n\r\n652\r\n\r\n\r\n654\r\n\r\n\r\n655\r\n\r\n\r\n656\r\n\r\n\r\n657\r\n\r\n\r\n658\r\n\r\n\r\n659\r\n\r\n\r\n661\r\n\r\n\r\n662\r\n\r\n\r\n663\r\n\r\n\r\n665\r\n\r\n\r\n666\r\n\r\n\r\n667\r\n\r\n\r\n669\r\n\r\n\r\n673\r\n\r\n\r\n674\r\n\r\n\r\n675\r\n\r\n\r\n676\r\n\r\n\r\n677\r\n\r\n\r\n678\r\n\r\n\r\n679\r\n\r\n\r\n680\r\n\r\n\r\n681\r\n\r\n\r\n683\r\n\r\n\r\n686\r\n\r\n\r\n687\r\n\r\n\r\n689\r\n\r\n\r\n690\r\n\r\n\r\n692\r\n\r\n\r\n693\r\n\r\n\r\n695\r\n\r\n\r\n696\r\n\r\n\r\n697\r\n\r\n\r\n698\r\n\r\n\r\n700\r\n\r\n\r\n702\r\n\r\n\r\n704\r\n\r\n\r\n705\r\n\r\n\r\n707\r\n\r\n\r\n708\r\n\r\n\r\n711\r\n\r\n\r\n713\r\n\r\n\r\n715\r\n\r\n\r\n716\r\n\r\n\r\n717\r\n\r\n\r\n719\r\n\r\n\r\n721\r\n\r\n\r\n722\r\n\r\n\r\n723\r\n\r\n\r\n724\r\n\r\n\r\n725\r\n\r\n\r\n727\r\n\r\n\r\n728\r\n\r\n\r\n731\r\n\r\n\r\n733\r\n\r\n\r\n734\r\n\r\n\r\n735\r\n\r\n\r\n739\r\n\r\n\r\n740\r\n\r\n\r\n742\r\n\r\n\r\n743\r\n\r\n\r\n744\r\n\r\n\r\n746\r\n\r\n\r\n747\r\n\r\n\r\n748\r\n\r\n\r\n749\r\n\r\n\r\n750\r\n\r\n\r\n751\r\n\r\n\r\n753\r\n\r\n\r\n755\r\n\r\n\r\n756\r\n\r\n\r\n757\r\n\r\n\r\n758\r\n\r\n\r\n760\r\n\r\n\r\n761\r\n\r\n\r\n762\r\n\r\n\r\n763\r\n\r\n\r\n765\r\n\r\n\r\n766\r\n\r\n\r\n767\r\n\r\n\r\n768\r\n\r\n\r\n769\r\n\r\n\r\n770\r\n\r\n\r\n771\r\n\r\n\r\n772\r\n\r\n\r\n774\r\n\r\n\r\n776\r\n\r\n\r\n777\r\n\r\n\r\n778\r\n\r\n\r\n779\r\n\r\n\r\n781\r\n\r\n\r\n783\r\n\r\n\r\n784\r\n\r\n\r\n786\r\n\r\n\r\n788\r\n\r\n\r\n790\r\n\r\n\r\n791\r\n\r\n\r\n792\r\n\r\n\r\n793\r\n\r\n\r\n794\r\n\r\n\r\n795\r\n\r\n\r\n796\r\n\r\n\r\n798\r\n\r\n\r\n799\r\n\r\n\r\n801\r\n\r\n\r\n805\r\n\r\n\r\n806\r\n\r\n\r\n809\r\n\r\n\r\n810\r\n\r\n\r\n812\r\n\r\n\r\n813\r\n\r\n\r\n816\r\n\r\n\r\n817\r\n\r\n\r\n818\r\n\r\n\r\n820\r\n\r\n\r\n822\r\n\r\n\r\n824\r\n\r\n\r\n827\r\n\r\n\r\n833\r\n\r\n\r\n835\r\n\r\n\r\n837\r\n\r\n\r\n838\r\n\r\n\r\n841\r\n\r\n\r\n842\r\n\r\n\r\n847\r\n\r\n\r\n848\r\n\r\n\r\n849\r\n\r\n\r\n852\r\n\r\n\r\n856\r\n\r\n\r\n857\r\n\r\n\r\n860\r\n\r\n\r\n862\r\n\r\n\r\n864\r\n\r\n\r\n866\r\n\r\n\r\n868\r\n\r\n\r\n869\r\n\r\n\r\n871\r\n\r\n\r\n872\r\n\r\n\r\n876\r\n\r\n\r\n878\r\n\r\n\r\n880\r\n\r\n\r\n882\r\n\r\n\r\n883\r\n\r\n\r\n884\r\n\r\n\r\n885\r\n\r\n\r\n886\r\n\r\n\r\n888\r\n\r\n\r\n889\r\n\r\n\r\n890\r\n\r\n\r\n893\r\n\r\n\r\n894\r\n\r\n\r\n896\r\n\r\n\r\n897\r\n\r\n\r\n901\r\n\r\n\r\n902\r\n\r\n\r\n903\r\n\r\n\r\n905\r\n\r\n\r\n906\r\n\r\n\r\n908\r\n\r\n\r\n909\r\n\r\n\r\n910\r\n\r\n\r\n912\r\n\r\n\r\n913\r\n\r\n\r\n915\r\n\r\n\r\n916\r\n\r\n\r\n918\r\n\r\n\r\n919\r\n\r\n\r\n920\r\n\r\n\r\n925\r\n\r\n\r\n927\r\n\r\n\r\n928\r\n\r\n\r\n929\r\n\r\n\r\n931\r\n\r\n\r\n933\r\n\r\n\r\n934\r\n\r\n\r\n938\r\n\r\n\r\n939\r\n\r\n\r\n942\r\n\r\n\r\n944\r\n\r\n\r\n946\r\n\r\n\r\n948\r\n\r\n\r\n951\r\n\r\n\r\n952\r\n\r\n\r\n953\r\n\r\n\r\n954\r\n\r\n\r\n955\r\n\r\n\r\n956\r\n\r\n\r\n957\r\n\r\n\r\n959\r\n\r\n\r\n960\r\n\r\n\r\n961\r\n\r\n\r\n963\r\n\r\n\r\n965\r\n\r\n\r\n966\r\n\r\n\r\n967\r\n\r\n\r\n968\r\n\r\n\r\n969\r\n\r\n\r\n971\r\n\r\n\r\n972\r\n\r\n\r\n973\r\n\r\n\r\n974\r\n\r\n\r\n975\r\n\r\n\r\n978\r\n\r\n\r\n980\r\n\r\n\r\n981\r\n\r\n\r\n983\r\n\r\n\r\n984\r\n\r\n\r\n985\r\n\r\n\r\n986\r\n\r\n\r\n987\r\n\r\n\r\n990\r\n\r\n\r\n995\r\n\r\n\r\n996\r\n\r\n\r\n997\r\n\r\n\r\n999\r\n\r\n\r\n1001\r\n\r\n\r\n1003\r\n\r\n\r\n1004\r\n\r\n\r\n1005\r\n\r\n\r\n1006\r\n\r\n\r\n1007\r\n\r\n\r\n1008\r\n\r\n\r\n1010\r\n\r\n\r\n1011\r\n\r\n\r\n1012\r\n\r\n\r\n1013\r\n\r\n\r\n1014\r\n\r\n\r\n1015\r\n\r\n\r\n1017\r\n\r\n\r\n1018\r\n\r\n\r\n1019\r\n\r\n\r\n1020\r\n\r\n\r\n1025\r\n\r\n\r\n1026\r\n\r\n\r\n1027\r\n\r\n\r\n1028\r\n\r\n\r\n1029\r\n\r\n\r\n1030\r\n\r\n\r\n1034\r\n\r\n\r\n1035\r\n\r\n\r\n1036\r\n\r\n\r\n1037\r\n\r\n\r\n1039\r\n\r\n\r\n1043\r\n\r\n\r\n1045\r\n\r\n\r\n1046\r\n\r\n\r\n1049\r\n\r\n\r\n1050\r\n\r\n\r\n1051\r\n\r\n\r\n1052\r\n\r\n\r\n1053\r\n\r\n\r\n1057\r\n\r\n\r\n1058\r\n\r\n\r\n1060\r\n\r\n\r\n1061\r\n\r\n\r\n1063\r\n\r\n\r\n1064\r\n\r\n\r\n1065\r\n\r\n\r\n1066\r\n\r\n\r\n1067\r\n\r\n\r\n1068\r\n\r\n\r\n1069\r\n\r\n\r\n1070\r\n\r\n\r\n1071\r\n\r\n\r\n1075\r\n\r\n\r\n1077\r\n\r\n\r\n1078\r\n\r\n\r\n1079\r\n\r\n\r\n1082\r\n\r\n\r\n1083\r\n\r\n\r\n1084\r\n\r\n\r\n1087\r\n\r\n\r\n1088\r\n\r\n\r\n1089\r\n\r\n\r\n1090\r\n\r\n\r\n1092\r\n\r\n\r\n1094\r\n\r\n\r\n1095\r\n\r\n\r\n1096\r\n\r\n\r\n1097\r\n\r\n\r\n1099\r\n\r\n\r\n1102\r\n\r\n\r\n1104\r\n\r\n\r\n1105\r\n\r\n\r\n1106\r\n\r\n\r\n1107\r\n\r\n\r\n1108\r\n\r\n\r\n1109\r\n\r\n\r\n1112\r\n\r\n\r\n1114\r\n\r\n\r\n1119\r\n\r\n\r\n1122\r\n\r\n\r\n1124\r\n\r\n\r\n1125\r\n\r\n\r\n1126\r\n\r\n\r\n1127\r\n\r\n\r\n1128\r\n\r\n\r\n1129\r\n\r\n\r\n1131\r\n\r\n\r\n1133\r\n\r\n\r\n1135\r\n\r\n\r\n1136\r\n\r\n\r\n1137\r\n\r\n\r\n1138\r\n\r\n\r\n1139\r\n\r\n\r\n1140\r\n\r\n\r\n1141\r\n\r\n\r\n1142\r\n\r\n\r\n1145\r\n\r\n\r\n1146\r\n\r\n\r\n1147\r\n\r\n\r\n1148\r\n\r\n\r\n1152\r\n\r\n\r\n1153\r\n\r\n\r\n1154\r\n\r\n\r\n1155\r\n\r\n\r\n1157\r\n\r\n\r\n1158\r\n\r\n\r\n1161\r\n\r\n\r\n1162\r\n\r\n\r\n1163\r\n\r\n\r\n1164\r\n\r\n\r\n1165\r\n\r\n\r\n1168\r\n\r\n\r\n1169\r\n\r\n\r\n1170\r\n\r\n\r\n1174\r\n\r\n\r\n1176\r\n\r\n\r\n1178\r\n\r\n\r\n1179\r\n\r\n\r\n1181\r\n\r\n\r\n1183\r\n\r\n\r\n1184\r\n\r\n\r\n1185\r\n\r\n\r\n1186\r\n\r\n\r\n1187\r\n\r\n\r\n1188\r\n\r\n\r\n1189\r\n\r\n\r\n1190\r\n\r\n\r\n1191\r\n\r\n\r\n1192\r\n\r\n\r\n1193\r\n\r\n\r\n1194\r\n\r\n\r\n1195\r\n\r\n\r\n1196\r\n\r\n\r\n1198\r\n\r\n\r\n1199\r\n\r\n\r\n1200\r\n\r\n\r\n1205\r\n\r\n\r\n1206\r\n\r\n\r\n1208\r\n\r\n\r\n1215\r\n\r\n\r\n1216\r\n\r\n\r\n1221\r\n\r\n\r\n1222\r\n\r\n\r\n1225\r\n\r\n\r\n1226\r\n\r\n\r\n1227\r\n\r\n\r\n1228\r\n\r\n\r\n1229\r\n\r\n\r\n1232\r\n\r\n\r\n1233\r\n\r\n\r\n1234\r\n\r\n\r\n1235\r\n\r\n\r\n1237\r\n\r\n\r\n1239\r\n\r\n\r\n1241\r\n\r\n\r\n1244\r\n\r\n\r\n1245\r\n\r\n\r\n1246\r\n\r\n\r\n1247\r\n\r\n\r\n1248\r\n\r\n\r\n1249\r\n\r\n\r\n1252\r\n\r\n\r\n1253\r\n\r\n\r\n1254\r\n\r\n\r\n1255\r\n\r\n\r\n1256\r\n\r\n\r\n1258\r\n\r\n\r\n1261\r\n\r\n\r\n1262\r\n\r\n\r\n1264\r\n\r\n\r\n1268\r\n\r\n\r\n1270\r\n\r\n\r\n1271\r\n\r\n\r\n1272\r\n\r\n\r\n1273\r\n\r\n\r\n1275\r\n\r\n\r\n1277\r\n\r\n\r\n1280\r\n\r\n\r\n1281\r\n\r\n\r\n1284\r\n\r\n\r\n1285\r\n\r\n\r\n1286\r\n\r\n\r\n1287\r\n\r\n\r\n1288\r\n\r\n\r\n1290\r\n\r\n\r\n1291\r\n\r\n\r\n1292\r\n\r\n\r\n1296\r\n\r\n\r\n1297\r\n\r\n\r\n1298\r\n\r\n\r\n1299\r\n\r\n\r\n1302\r\n\r\n\r\n1303\r\n\r\n\r\n1307\r\n\r\n\r\n1308\r\n\r\n\r\n1309\r\n\r\n\r\n1310\r\n\r\n\r\n1311\r\n\r\n\r\n1313\r\n\r\n\r\n1314\r\n\r\n\r\n1315\r\n\r\n\r\n1316\r\n\r\n\r\n1318\r\n\r\n\r\n1319\r\n\r\n\r\n1322\r\n\r\n\r\n1323\r\n\r\n\r\n1324\r\n\r\n\r\n1325\r\n\r\n\r\n1327\r\n\r\n\r\n1329\r\n\r\n\r\n1330\r\n\r\n\r\n1332\r\n\r\n\r\n1333\r\n\r\n\r\n1335\r\n\r\n\r\n1337\r\n\r\n\r\n1338\r\n\r\n\r\n1339\r\n\r\n\r\n1341\r\n\r\n\r\n1342\r\n\r\n\r\n1344\r\n\r\n\r\n1347\r\n\r\n\r\n1348\r\n\r\n\r\n1350\r\n\r\n\r\n1351\r\n\r\n\r\n1352\r\n\r\n\r\n1353\r\n\r\n\r\n1355\r\n\r\n\r\n1357\r\n\r\n\r\n1358\r\n\r\n\r\n1359\r\n\r\n\r\n1361\r\n\r\n\r\n1362\r\n\r\n\r\n1364\r\n\r\n\r\n1367\r\n\r\n\r\n1368\r\n\r\n\r\n1373\r\n\r\n\r\n1374\r\n\r\n\r\n1377\r\n\r\n\r\n1378\r\n\r\n\r\n1379\r\n\r\n\r\n1380\r\n\r\n\r\n1382\r\n\r\n\r\n1383\r\n\r\n\r\n1384\r\n\r\n\r\n1385\r\n\r\n\r\n1387\r\n\r\n\r\n1389\r\n\r\n\r\n1390\r\n\r\n\r\n1391\r\n\r\n\r\n1392\r\n\r\n\r\n1394\r\n\r\n\r\n1396\r\n\r\n\r\n1399\r\n\r\n\r\n1401\r\n\r\n\r\n1402\r\n\r\n\r\n1403\r\n\r\n\r\n1404\r\n\r\n\r\n1406\r\n\r\n\r\n1408\r\n\r\n\r\n1412\r\n\r\n\r\n1414\r\n\r\n\r\n1415\r\n\r\n\r\n1416\r\n\r\n\r\n1417\r\n\r\n\r\n1422\r\n\r\n\r\n1423\r\n\r\n\r\n1426\r\n\r\n\r\n1427\r\n\r\n\r\n1429\r\n\r\n\r\n1431\r\n\r\n\r\n1432\r\n\r\n\r\n1433\r\n\r\n\r\n1434\r\n\r\n\r\n1436\r\n\r\n\r\n1438\r\n\r\n\r\n1439\r\n\r\n\r\n1440\r\n\r\n\r\n1441\r\n\r\n\r\n1443\r\n\r\n\r\n1445\r\n\r\n\r\n1446\r\n\r\n\r\n1448\r\n\r\n\r\n1450\r\n\r\n\r\n1451\r\n\r\n\r\n1452\r\n\r\n\r\n1453\r\n\r\n\r\n1454\r\n\r\n\r\n1458\r\n\r\n\r\n1459\r\n\r\n\r\n1460\r\n\r\n\r\n1461\r\n\r\n\r\n1463\r\n\r\n\r\n1464\r\n\r\n\r\n1467\r\n\r\n\r\n1470\r\n\r\n\r\n1472\r\n\r\n\r\n1473\r\n\r\n\r\n1475\r\n\r\n\r\n1477\r\n\r\n\r\n1479\r\n\r\n\r\n1480\r\n\r\n\r\n1481\r\n\r\n\r\n1482\r\n\r\n\r\n1483\r\n\r\n\r\n1484\r\n\r\n\r\n1487\r\n\r\n\r\n1492\r\n\r\n\r\n1496\r\n\r\n\r\n1501\r\n\r\n\r\n1503\r\n\r\n\r\n1504\r\n\r\n\r\n1506\r\n\r\n\r\n1507\r\n\r\n\r\n1508\r\n\r\n\r\n1510\r\n\r\n\r\n1511\r\n\r\n\r\n1515\r\n\r\n\r\n1517\r\n\r\n\r\n1519\r\n\r\n\r\n1520\r\n\r\n\r\n1522\r\n\r\n\r\n1523\r\n\r\n\r\n1524\r\n\r\n\r\n1525\r\n\r\n\r\n1527\r\n\r\n\r\n1528\r\n\r\n\r\n1530\r\n\r\n\r\n1531\r\n\r\n\r\n1532\r\n\r\n\r\n1533\r\n\r\n\r\n1534\r\n\r\n\r\n1535\r\n\r\n\r\n1537\r\n\r\n\r\n1538\r\n\r\n\r\n1539\r\n\r\n\r\n1540\r\n\r\n\r\n1541\r\n\r\n\r\n1544\r\n\r\n\r\n1546\r\n\r\n\r\n1547\r\n\r\n\r\n1548\r\n\r\n\r\n1553\r\n\r\n\r\n1556\r\n\r\n\r\n1557\r\n\r\n\r\n1558\r\n\r\n\r\n1559\r\n\r\n\r\n1560\r\n\r\n\r\n1561\r\n\r\n\r\n1564\r\n\r\n\r\n1565\r\n\r\n\r\n1566\r\n\r\n\r\n1567\r\n\r\n\r\n1568\r\n\r\n\r\n1570\r\n\r\n\r\n1571\r\n\r\n\r\n1572\r\n\r\n\r\n1574\r\n\r\n\r\n1576\r\n\r\n\r\n1577\r\n\r\n\r\n1579\r\n\r\n\r\n1582\r\n\r\n\r\n1583\r\n\r\n\r\n1584\r\n\r\n\r\n1589\r\n\r\n\r\n1590\r\n\r\n\r\n1592\r\n\r\n\r\n1595\r\n\r\n\r\n1596\r\n\r\n\r\n1600\r\n\r\n\r\n1601\r\n\r\n\r\n1603\r\n\r\n\r\n1604\r\n\r\n\r\n1605\r\n\r\n\r\n1607\r\n\r\n\r\n1609\r\n\r\n\r\n1611\r\n\r\n\r\n1613\r\n\r\n\r\n1614\r\n\r\n\r\n1615\r\n\r\n\r\n1616\r\n\r\n\r\n1620\r\n\r\n\r\n1622\r\n\r\n\r\n1624\r\n\r\n\r\n1627\r\n\r\n\r\n1628\r\n\r\n\r\n1631\r\n\r\n\r\n1633\r\n\r\n\r\n1637\r\n\r\n\r\n1638\r\n\r\n\r\n1641\r\n\r\n\r\n1642\r\n\r\n\r\n1643\r\n\r\n\r\n1644\r\n\r\n\r\n1645\r\n\r\n\r\n1647\r\n\r\n\r\n1648\r\n\r\n\r\n1649\r\n\r\n\r\n1652\r\n\r\n\r\n1653\r\n\r\n\r\n1654\r\n\r\n\r\n1657\r\n\r\n\r\n1659\r\n\r\n\r\n1662\r\n\r\n\r\n1664\r\n\r\n\r\n1665\r\n\r\n\r\n1669\r\n\r\n\r\n1670\r\n\r\n\r\n1671\r\n\r\n\r\n1673\r\n\r\n\r\n1676\r\n\r\n\r\n1678\r\n\r\n\r\n1679\r\n\r\n\r\n1681\r\n\r\n\r\n1682\r\n\r\n\r\n1684\r\n\r\n\r\n1686\r\n\r\n\r\n1688\r\n\r\n\r\n1690\r\n\r\n\r\n1693\r\n\r\n\r\n1695\r\n\r\n\r\n1696\r\n\r\n\r\n1697\r\n\r\n\r\n1700\r\n\r\n\r\n1702\r\n\r\n\r\n1704\r\n\r\n\r\n1705\r\n\r\n\r\n1708\r\n\r\n\r\n1709\r\n\r\n\r\n1710\r\n\r\n\r\n1711\r\n\r\n\r\n1712\r\n\r\n\r\n1713\r\n\r\n\r\n1714\r\n\r\n\r\n1716\r\n\r\n\r\n1717\r\n\r\n\r\n1722\r\n\r\n\r\n1723\r\n\r\n\r\n1724\r\n\r\n\r\n1727\r\n\r\n\r\n1728\r\n\r\n\r\n1730\r\n\r\n\r\n1732\r\n\r\n\r\n1734\r\n\r\n\r\n1735\r\n\r\n\r\n1736\r\n\r\n\r\n1737\r\n\r\n\r\n1738\r\n\r\n\r\n1739\r\n\r\n\r\n1742\r\n\r\n\r\n1743\r\n\r\n\r\n1744\r\n\r\n\r\n1746\r\n\r\n\r\n1747\r\n\r\n\r\n1748\r\n\r\n\r\n1750\r\n\r\n\r\n1751\r\n\r\n\r\n1752\r\n\r\n\r\n1754\r\n\r\n\r\n1755\r\n\r\n\r\n1756\r\n\r\n\r\n1758\r\n\r\n\r\n1760\r\n\r\n\r\n1761\r\n\r\n\r\n1762\r\n\r\n\r\n1767\r\n\r\n\r\n1768\r\n\r\n\r\n1770\r\n\r\n\r\n1771\r\n\r\n\r\n1772\r\n\r\n\r\n1773\r\n\r\n\r\n1777\r\n\r\n\r\n1778\r\n\r\n\r\n1782\r\n\r\n\r\n1783\r\n\r\n\r\n1787\r\n\r\n\r\n1788\r\n\r\n\r\n1789\r\n\r\n\r\n1794\r\n\r\n\r\n1795\r\n\r\n\r\n1796\r\n\r\n\r\n1797\r\n\r\n\r\n1798\r\n\r\n\r\n1799\r\n\r\n\r\n1800\r\n\r\n\r\n1803\r\n\r\n\r\n1804\r\n\r\n\r\n1808\r\n\r\n\r\n1809\r\n\r\n\r\n1810\r\n\r\n\r\n1811\r\n\r\n\r\n1813\r\n\r\n\r\n1814\r\n\r\n\r\n1816\r\n\r\n\r\n1817\r\n\r\n\r\n1820\r\n\r\n\r\n1821\r\n\r\n\r\n1822\r\n\r\n\r\n1823\r\n\r\n\r\n1824\r\n\r\n\r\n1825\r\n\r\n\r\n1827\r\n\r\n\r\n1828\r\n\r\n\r\n1829\r\n\r\n\r\n1830\r\n\r\n\r\n1833\r\n\r\n\r\n1836\r\n\r\n\r\n1838\r\n\r\n\r\n1839\r\n\r\n\r\n1841\r\n\r\n\r\n1842\r\n\r\n\r\n1843\r\n\r\n\r\n1844\r\n\r\n\r\n1845\r\n\r\n\r\n1848\r\n\r\n\r\n1849\r\n\r\n\r\n1853\r\n\r\n\r\n1854\r\n\r\n\r\n1855\r\n\r\n\r\n1857\r\n\r\n\r\n1858\r\n\r\n\r\n1860\r\n\r\n\r\n1862\r\n\r\n\r\n1864\r\n\r\n\r\n1866\r\n\r\n\r\n1867\r\n\r\n\r\n1868\r\n\r\n\r\n1869\r\n\r\n\r\n1871\r\n\r\n\r\n1872\r\n\r\n\r\n1875\r\n\r\n\r\n1877\r\n\r\n\r\n1878\r\n\r\n\r\n1880\r\n\r\n\r\n1881\r\n\r\n\r\n1883\r\n\r\n\r\n1887\r\n\r\n\r\n1888\r\n\r\n\r\n1889\r\n\r\n\r\n1890\r\n\r\n\r\n1894\r\n\r\n\r\n1895\r\n\r\n\r\n1896\r\n\r\n\r\n1900\r\n\r\n\r\n1902\r\n\r\n\r\n1904\r\n\r\n\r\n1907\r\n\r\n\r\n1908\r\n\r\n\r\n1909\r\n\r\n\r\n1910\r\n\r\n\r\n1911\r\n\r\n\r\n1912\r\n\r\n\r\n1913\r\n\r\n\r\n1914\r\n\r\n\r\n1916\r\n\r\n\r\n1919\r\n\r\n\r\n1920\r\n\r\n\r\n1921\r\n\r\n\r\n1923\r\n\r\n\r\n1924\r\n\r\n\r\n1925\r\n\r\n\r\n1928\r\n\r\n\r\n1929\r\n\r\n\r\n1930\r\n\r\n\r\n1932\r\n\r\n\r\n1933\r\n\r\n\r\n1934\r\n\r\n\r\n1935\r\n\r\n\r\n1936\r\n\r\n\r\n1937\r\n\r\n\r\n1938\r\n\r\n\r\n1940\r\n\r\n\r\n1941\r\n\r\n\r\n1942\r\n\r\n\r\n1945\r\n\r\n\r\n1946\r\n\r\n\r\n1947\r\n\r\n\r\n1951\r\n\r\n\r\n1952\r\n\r\n\r\n1955\r\n\r\n\r\n1957\r\n\r\n\r\n1958\r\n\r\n\r\n1960\r\n\r\n\r\n1962\r\n\r\n\r\n1965\r\n\r\n\r\n1966\r\n\r\n\r\n1967\r\n\r\n\r\n1968\r\n\r\n\r\n1973\r\n\r\n\r\n1974\r\n\r\n\r\n1976\r\n\r\n\r\n1977\r\n\r\n\r\n1978\r\n\r\n\r\n1979\r\n\r\n\r\n1981\r\n\r\n\r\n1983\r\n\r\n\r\n1984\r\n\r\n\r\n1986\r\n\r\n\r\n1987\r\n\r\n\r\n1990\r\n\r\n\r\n1993\r\n\r\n\r\n1994\r\n\r\n\r\n1995\r\n\r\n\r\n1998\r\n\r\n\r\n1999\r\n\r\n\r\n2000\r\n\r\n\r\n2001\r\n\r\n\r\n2002\r\n\r\n\r\n2004\r\n\r\n\r\n2006\r\n\r\n\r\n2007\r\n\r\n\r\n2012\r\n\r\n\r\n2013\r\n\r\n\r\n2014\r\n\r\n\r\n2016\r\n\r\n\r\n2017\r\n\r\n\r\n2019\r\n\r\n\r\n2020\r\n\r\n\r\n2022\r\n\r\n\r\n2023\r\n\r\n\r\n2026\r\n\r\n\r\n2027\r\n\r\n\r\n2029\r\n\r\n\r\n2031\r\n\r\n\r\n2032\r\n\r\n\r\n2037\r\n\r\n\r\n2040\r\n\r\n\r\n2041\r\n\r\n\r\n2042\r\n\r\n\r\n2043\r\n\r\n\r\n2045\r\n\r\n\r\n2046\r\n\r\n\r\n2048\r\n\r\n\r\n2049\r\n\r\n\r\n2060\r\n\r\n\r\n2063\r\n\r\n\r\n2065\r\n\r\n\r\n2068\r\n\r\n\r\n2069\r\n\r\n\r\n\r\n#grab the data\r\n#take these rows\r\nFuelTrain <- Features_data_set[ trainIndex,]\r\n#don't take these rows\r\nFuelTest  <- Features_data_set[-trainIndex,]\r\n\r\n#we now have training and testing data sets\r\n\r\n\r\n\r\n\r\n\r\n#for the algorithm we are using it requires that we standardize\r\n\r\n#center= subtract the means and \r\n#scale = divide by standard deviation\r\n#?preProcess sets the conversion up \r\npreProcValues <- preProcess(FuelTrain, method = c(\"center\", \"scale\"))\r\n\r\npreProcValues\r\n\r\n\r\nCreated from 1242 samples and 12 variables\r\n\r\nPre-processing:\r\n  - centered (10)\r\n  - ignored (2)\r\n  - scaled (10)\r\n\r\n\r\n\r\n#this predict actual change the variables\r\ntrainTransformed <- predict(preProcValues, FuelTrain)\r\n#repeat for testing\r\npreProcValues <- preProcess(FuelTest, method = c(\"center\", \"scale\"))\r\ntestTransformed <- predict(preProcValues, FuelTest)\r\n\r\n\r\n\r\n\r\n\r\n#data is split and standardized\r\n#time to run the model\r\n#fit knn\r\n\r\n#use train function and set up the equation\r\nknn_fit<-train(Fuel_Price~Unemployment,\r\n              #data we are using\r\n              data=trainTransformed,\r\n              #algorithm we are using\r\n              method=\"knn\",\r\n              #the hyper parameter or tuning parameter is the\r\n              #number of neighbors...here we set it to 5\r\n              tuneGrid=data.frame(k=5))\r\n\r\n#this is the object that holds the model\r\nknn_fit\r\n\r\n\r\nk-Nearest Neighbors \r\n\r\n1242 samples\r\n   1 predictor\r\n\r\nNo pre-processing\r\nResampling: Bootstrapped (25 reps) \r\nSummary of sample sizes: 1242, 1242, 1242, 1242, 1242, 1242, ... \r\nResampling results:\r\n\r\n  RMSE       Rsquared   MAE      \r\n  0.7889838  0.4057207  0.6361757\r\n\r\nTuning parameter 'k' was held constant at a value of 5\r\n\r\n#predict on the test set\r\nknn_pred<-predict(knn_fit,testTransformed)\r\n\r\n\r\n\r\n\r\n\r\n#fit simple linear regression model\r\nmodel_int <- lm(Unemployment ~ Unemployment , data = FuelTrain)\r\n\r\nint_results<-predict(model_int,FuelTest)\r\n###compute fit\r\nsummary(model_int)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Unemployment ~ Unemployment, data = FuelTrain)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.5466 -1.0986 -0.0336  1.0084  5.6594 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  7.23061    0.04767   151.7   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.68 on 1241 degrees of freedom\r\n\r\nknitr::kable(caret::RMSE(int_results,FuelTest$Fuel_Price),col.names = \"RMSE Test\")\r\n\r\n\r\n\r\nRMSE Test\r\n\r\n\r\n3.642862\r\n\r\n\r\nknitr::kable(caret::RMSE(int_results,FuelTrain$Fuel_Price),col.names = \"RMSE Train\")\r\n\r\n\r\n\r\nRMSE Train\r\n\r\n\r\n3.639817\r\n\r\n\r\nFuelTest$Sample<-\"Testing\"\r\nFuelTrain$Sample<-\"Training\"\r\n\r\nCombined_Fuel<-rbind(FuelTest,FuelTrain)\r\n#create regression plot with customized style\r\nggplot(Combined_Fuel,aes(x=Fuel_Price, y=Unemployment,color=Sample)) +\r\n  geom_point(alpha=.5) +\r\n  theme_minimal() +\r\n  labs(x='Fuel Price', y='Unemployment', title='Linear Regression Plot') +\r\n  theme(plot.title = element_text(hjust=0.5, size=20, face='bold')) +\r\n  geom_abline(aes(slope=model_int$coefficients[[1]],intercept=0),color=\"red\")\r\n\r\n\r\n\r\n\r\nThe Linear Regression Model shows that the data is extremely scattered and clustered to form any true conclusion from the data. This Model and the one above both show outliers in the same areas. Also, the RMSE are nearly identical for the Training data and the testing data. This tells me that this is a bad set of data to use for analysis.\r\n\r\n\r\nlibrary(tidyverse)\r\n#create residuals\r\ntestwithpred<-as.data.frame(cbind(int_results,FuelTest))\r\n#create residuals\r\ntestwithpred<-testwithpred%>%\r\n  rename(prediction=int_results)%>%\r\n  mutate(error=Fuel_Price-prediction)\r\n\r\n#create regression plot with customized style\r\nggplot(testwithpred,aes(x=Fuel_Price, y=error)) +\r\n  geom_point(alpha=.5,color=\"deepskyblue\") +\r\n  theme_minimal() +\r\n  labs(x='Fuel_Price', y='Error', title='Regression Error Plot') +\r\n  theme(plot.title = element_text(hjust=0.25, size=20, face='bold')) +\r\n  geom_hline(yintercept=0,color=\"red\",linetype=\"dashed\")\r\n\r\n\r\n\r\n\r\nThe Regression Error Plot shows that the data in this data set creates a bad model for linear regression.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-04T14:47:26-06:00"
    },
    {
      "path": "index.html",
      "title": "Chrissy Farish",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          The most awesome website ever!\r\n          \r\n          \r\n          Home\r\n          Resume\r\n          \r\n          \r\n          Projects\r\n           \r\n          ▾\r\n          \r\n          \r\n          R Squared\r\n          Machine Learning Model\r\n          Final Project\r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Chrissy Farish\r\n            \r\n            \r\n              \r\n                \r\n                    \r\n                      \r\n                        LinkedIn\r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                        GitHub\r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                        Email\r\n                      \r\n                    \r\n                  \r\n                                  \r\n            \r\n          \r\n        \r\n        \r\n        \r\n          \r\n            I am a trained accountant living in the Mississippi Delta area. Currently I working on my Masters of Professional Accounting degree with a Minor in Data Analytics. I love talking about books, cats, and hunting.\r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Chrissy Farish\r\n            \r\n            \r\n              \r\n                \r\n                                    \r\n                    \r\n                      LinkedIn\r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      GitHub\r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      Email\r\n                    \r\n                  \r\n                                  \r\n              \r\n            \r\n            \r\n              I am a trained accountant living in the Mississippi Delta area. Currently I working on my Masters of Professional Accounting degree with a Minor in Data Analytics. I love talking about books, cats, and hunting.\r\n              \r\n            \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2021-12-03T15:23:03-06:00"
    },
    {
      "path": "Machine_Learning_Model.html",
      "title": "Machine Learning Model",
      "description": "Linear Regression\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nLinear regression is simply using various types of data to determine which one is the best one to use when making predictions. Then it uses a graph to determine how well the prediction is. Finally, it draws a line through a scattergraph so you can see how close the data gets to the line. This can be useful in accounting during an audit. It can predict which areas will need the most areas of audit.\r\nChange the p=.6 to p=.75 in the Data Pre-Processing section. How did the classification results change?\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(caret)\r\n#change the p=.6 to p=.75\r\n#what does createDataPartition do?\r\n#run the following code: ?createDataPartition\r\n\r\n#this creates an index of rows in include in the training\r\n#literally lists the rows to keep\r\ntrainIndex <- createDataPartition(iris$Species, p = .75, list = FALSE, times = 1)\r\n\r\n#rows to keep\r\nknitr::kable(trainIndex)%>%\r\n  kableExtra::kable_styling(\"striped\")%>%\r\n  kableExtra::scroll_box(width = \"100%\",height=\"200px\")\r\n\r\n\r\n\r\n\r\nResample1\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n8\r\n\r\n\r\n9\r\n\r\n\r\n10\r\n\r\n\r\n12\r\n\r\n\r\n13\r\n\r\n\r\n14\r\n\r\n\r\n16\r\n\r\n\r\n18\r\n\r\n\r\n20\r\n\r\n\r\n21\r\n\r\n\r\n22\r\n\r\n\r\n23\r\n\r\n\r\n25\r\n\r\n\r\n26\r\n\r\n\r\n27\r\n\r\n\r\n28\r\n\r\n\r\n31\r\n\r\n\r\n32\r\n\r\n\r\n33\r\n\r\n\r\n34\r\n\r\n\r\n35\r\n\r\n\r\n36\r\n\r\n\r\n38\r\n\r\n\r\n39\r\n\r\n\r\n40\r\n\r\n\r\n41\r\n\r\n\r\n42\r\n\r\n\r\n43\r\n\r\n\r\n44\r\n\r\n\r\n45\r\n\r\n\r\n46\r\n\r\n\r\n47\r\n\r\n\r\n51\r\n\r\n\r\n52\r\n\r\n\r\n54\r\n\r\n\r\n56\r\n\r\n\r\n58\r\n\r\n\r\n59\r\n\r\n\r\n60\r\n\r\n\r\n61\r\n\r\n\r\n63\r\n\r\n\r\n64\r\n\r\n\r\n65\r\n\r\n\r\n66\r\n\r\n\r\n68\r\n\r\n\r\n69\r\n\r\n\r\n70\r\n\r\n\r\n71\r\n\r\n\r\n72\r\n\r\n\r\n73\r\n\r\n\r\n74\r\n\r\n\r\n75\r\n\r\n\r\n77\r\n\r\n\r\n78\r\n\r\n\r\n79\r\n\r\n\r\n81\r\n\r\n\r\n82\r\n\r\n\r\n84\r\n\r\n\r\n86\r\n\r\n\r\n87\r\n\r\n\r\n88\r\n\r\n\r\n89\r\n\r\n\r\n90\r\n\r\n\r\n91\r\n\r\n\r\n93\r\n\r\n\r\n94\r\n\r\n\r\n95\r\n\r\n\r\n96\r\n\r\n\r\n97\r\n\r\n\r\n98\r\n\r\n\r\n101\r\n\r\n\r\n102\r\n\r\n\r\n103\r\n\r\n\r\n104\r\n\r\n\r\n105\r\n\r\n\r\n106\r\n\r\n\r\n107\r\n\r\n\r\n108\r\n\r\n\r\n109\r\n\r\n\r\n112\r\n\r\n\r\n114\r\n\r\n\r\n115\r\n\r\n\r\n116\r\n\r\n\r\n117\r\n\r\n\r\n118\r\n\r\n\r\n119\r\n\r\n\r\n122\r\n\r\n\r\n123\r\n\r\n\r\n125\r\n\r\n\r\n126\r\n\r\n\r\n127\r\n\r\n\r\n128\r\n\r\n\r\n129\r\n\r\n\r\n131\r\n\r\n\r\n132\r\n\r\n\r\n134\r\n\r\n\r\n135\r\n\r\n\r\n136\r\n\r\n\r\n139\r\n\r\n\r\n140\r\n\r\n\r\n141\r\n\r\n\r\n142\r\n\r\n\r\n144\r\n\r\n\r\n145\r\n\r\n\r\n146\r\n\r\n\r\n147\r\n\r\n\r\n149\r\n\r\n\r\n150\r\n\r\n\r\n\r\n#grab the data\r\n#take these rows\r\nirisTrain <- iris[ trainIndex,]\r\n#don't take these rows\r\nirisTest  <- iris[-trainIndex,]\r\n\r\n#we now have training and testing data sets\r\n\r\n\r\n\r\n\r\n\r\n#for the algorithm we are using it requires that we standardize\r\n\r\n#center= subtract the means and \r\n#scale = divide by standard deviation\r\n#?preProcess sets the conversion up \r\npreProcValues <- preProcess(irisTrain, method = c(\"center\", \"scale\"))\r\n\r\npreProcValues\r\n\r\n\r\nCreated from 114 samples and 5 variables\r\n\r\nPre-processing:\r\n  - centered (4)\r\n  - ignored (1)\r\n  - scaled (4)\r\n\r\n\r\n\r\n#this predict actual change the variables\r\ntrainTransformed <- predict(preProcValues, irisTrain)\r\n#repeat for testing\r\npreProcValues <- preProcess(irisTest, method = c(\"center\", \"scale\"))\r\ntestTransformed <- predict(preProcValues, irisTest)\r\n\r\n\r\n\r\n\r\n\r\n#data is split and standardized\r\n#time to run the model\r\n#fit knn\r\n\r\n#use train function and set up the equation\r\nknn_fit<-train(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,\r\n              #data we are using\r\n              data=trainTransformed,\r\n              #algorithm we are using\r\n              method=\"knn\",\r\n              #the hyperparameter or tuning parameter is the\r\n              #number of neighbors...here we set it to 5\r\n              tuneGrid=data.frame(k=5))\r\n\r\n#this is the object that holds the model\r\nknn_fit\r\n\r\n\r\nk-Nearest Neighbors \r\n\r\n114 samples\r\n  4 predictor\r\n  3 classes: 'setosa', 'versicolor', 'virginica' \r\n\r\nNo pre-processing\r\nResampling: Bootstrapped (25 reps) \r\nSummary of sample sizes: 114, 114, 114, 114, 114, 114, ... \r\nResampling results:\r\n\r\n  Accuracy   Kappa    \r\n  0.9343552  0.8997843\r\n\r\nTuning parameter 'k' was held constant at a value of 5\r\n\r\n#predict on the test set\r\nknn_pred<-predict(knn_fit,testTransformed)\r\n\r\n#confusion matrix gives us the results\r\nconfusionMatrix(knn_pred,testTransformed$Species)\r\n\r\n\r\nConfusion Matrix and Statistics\r\n\r\n            Reference\r\nPrediction   setosa versicolor virginica\r\n  setosa         12          0         0\r\n  versicolor      0         12         1\r\n  virginica       0          0        11\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9722          \r\n                 95% CI : (0.8547, 0.9993)\r\n    No Information Rate : 0.3333          \r\n    P-Value [Acc > NIR] : 4.864e-16       \r\n                                          \r\n                  Kappa : 0.9583          \r\n                                          \r\n Mcnemar's Test P-Value : NA              \r\n\r\nStatistics by Class:\r\n\r\n                     Class: setosa Class: versicolor Class: virginica\r\nSensitivity                 1.0000            1.0000           0.9167\r\nSpecificity                 1.0000            0.9583           1.0000\r\nPos Pred Value              1.0000            0.9231           1.0000\r\nNeg Pred Value              1.0000            1.0000           0.9600\r\nPrevalence                  0.3333            0.3333           0.3333\r\nDetection Rate              0.3333            0.3333           0.3056\r\nDetection Prevalence        0.3333            0.3611           0.3056\r\nBalanced Accuracy           1.0000            0.9792           0.9583\r\n\r\nBy comparing the variances we see that as the number of components increase each individual component’s explained variance drops.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-04T14:47:27-06:00"
    },
    {
      "path": "Resume.html",
      "title": "Chrissy Farish",
      "description": "Resume\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nCareer Objective\r\nAn accounting position utilizing previous experience and knowledge in a business environment.\r\nSkills\r\nSkilled in all aspects of office administration, organization of filing systems, use of electronic office equipment, handling multi-line phone systems, reception, data entry, and banking. Consistently noted by managers and staff for superior job performance and timely completion of all assignments. Knowledgeable in computer operations and applications including Banner – Finance, Human Resources, Accounts Receivable; Cognos Reporting Software, A/S400, Windows, QuickBooks Pro, Microsoft Word, Excel, Access, PowerPoint, WordPerfect, Internet, E-mail, and basic typing and keyboarding skills. Introduced to VLOOKUP in Excel, Tableau, and R Studio.\r\nExperience\r\n2016 – 2018 Assistant Comptroller Delta State University Cleveland, MS\r\nData entry of majority of the journal entries\r\nReconcile multiple bank statements that had not been reconciled to zero in years\r\nPrint Accounts Payable and Payroll checks\r\nWorked the outstanding check listing for Payroll and Accounts Payable\r\nProcessed online student refunds and travel/reimbursement checks to faculty and staff\r\nFile and organize journal entries\r\nAssist with work-study monthly reconciliation\r\n2016 – 2016 Staff Accountant Fred T. Neely & Company Ruleville, MS\r\nProcess Weekly/Biweekly/Monthly/Semi-Monthly Payrolls\r\nMake 941, 940, State Withholding, State Unemployment deposits\r\nProcess sales tax reports and payments\r\nAssist in the preparation of income tax returns\r\nReconcile Bank Statements\r\nProcess invoices through accounts payable\r\nPrepare 941 Quarterly Reports\r\nPrepare 940 Quarterly Reports\r\nEnroll employees for a certain client into Blue Cross Blue Shield\r\n2015 – 2016 Accountant I University of Mississippi Oxford, MS\r\nEnter manual paybacks and disbursements into two separate software system\r\nReconcile Miscellaneous Scholarship Account\r\nCreate and maintain Aid Id’s\r\nKey memos to student packages\r\nEmail students about paybacks and disbursements\r\n2008 - 2015 Staff Accountant Mississippi Delta Community College Moorhead, MS\r\nPrepare, verify, and control the entering of journal entries and adjustments to all funds\r\nCode receipts and disbursements\r\nEdit and post to general ledger\r\nReconcile bank statements for all funds\r\nFile accounting documents journal adjustments, general ledger adjustments, etc.\r\nReconciling and transferring monies and deposits between funds\r\nPrepare 941 Quarterly Reports\r\nAssist in daily cash count\r\nAssist with the Fall, Spring and Summer registration of students\r\nUtilize Excel to prepare detail spreadsheets on various funds\r\nPrepare monthly sales tax report\r\nProcess Biweekly and Workstudy payrolls\r\nPrepare Indirect Cost Rate\r\nAssist with End of Year Reconciliation\r\nDevelop various reports using Cognos\r\nEstablish and maintain credit card processing systems for the Business Office and bookstores on all campuses\r\nUploading files to the bank for student account refunds\r\nAssist Accounts Payable Manager in updating W-9 information for all vendors.\r\nPrepare daily deposits for bookstore\r\n2007 - 2008 Student Accounts Manager Mississippi Delta Community College Moorhead, MS\r\nInvolved in all aspects of customer service for the students including solving problems, accessing customer accounts, and receipting student payments.\r\nInvoicing of MPACT, Americorp, hospital scholarships, National Guard assistance, and various other scholarships\r\nReconciliation of scholarship funds\r\nWorked with various offices on campus to ensure proper student account fees being processed.\r\nDeveloping various reports using queries in A/S400.\r\n2001 - 2007 Office Manager Gardner Engineering, P.A. Indianola, MS\r\nManaged weekly payroll process for 25 employees in two company locations.\r\nProcessed all payroll deductions and filing SIMPLE IRA, medical, and garnishments\r\nManaged Accounts Receivable and Accounts Payable for the entire company\r\nAdministered employee payroll, company taxes, and maintained IRA Retirement Plan\r\nAuthorized petty cash, check distributions, and bank deposits\r\nPrepared various tax forms including Forms 941, 940, 1099-MISC, W2, and W3\r\nPerformed monthly bank reconciliation.\r\nPrepared numerous weekly internal reports for company officers\r\nHuman Resource administration\r\nEnrolled employees into benefit plans and SIMPLE IRA enrollments\r\n1997 - 2001 Secretary Gardner Engineering, P.A. Indianola, MS\r\nAccomplished general office duties\r\nCreated State Aid required forms in Microsoft Excel\r\nAssisted with Monthly Estimate Billings\r\nMaintained daily diaries on various state and municipality jobs • Supervised the reorganization of the central office filing system\r\n1995 - 1997 Pharmacy Technician/Clerk Fred’s Pharmacy Indianola, MS\r\nResponsible for maintaining adequate levels of inventory\r\nManaged sales transactions\r\nAssisted Pharmacists in filling prescriptions\r\nEducation\r\n2001 B.B.A., Accounting\r\nDelta State University\r\nCleveland, MS\r\n2003 M.B.A., Business Administration\r\nDelta State University\r\nCleveland, MS\r\n2018 Accounting Bridge Program\r\nAuburn University Online\r\nAuburn, AL\r\n2021 Masters of Professional Accountancy\r\nMississippi State University Online\r\nStarkville, MS\r\nActivities and Honors\r\nDean’s List\r\nPresident’s List\r\nMember, Student Accounting and Business Association\r\nMember, Accounting Honor Society\r\nMember, Career Day Service Ambassador\r\nMember, Delta Mu Delta. Business Honor Society\r\nWorkshops and Seminars\r\nPayroll Basics in Mississippi, Lorman Education Services, Jackson, MS, 2003. A one-day seminar designed to increase the participant’s knowledge of payroll laws and improve their ability to effectively manage and administer the payroll.\r\nThe Essentials of Payroll Management, Rockhurst University Continuing Education Center, Inc., Jackson, MS, 2002. An intensive one-day workshop and legal update for payroll professionals.\r\nReferences\r\nLaura Chisolm\r\nAccounts Payable/Receivable Specialist\r\nPearl River Community College\r\n601-403-1208lchisolm@prcc.edu\r\nSheron Burford Bookkeeper\r\nWashington School\r\n662-334-4096sburford@generals.ws\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-02T16:03:58-06:00"
    },
    {
      "path": "RSquared.html",
      "title": "R Squared",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nThe really interesting thing about R-Squared is that many people do not know that this figure is irrelevant. Many staticians use this calcuation a lot even though it is meaningless.\r\nR-squared is a statistic that often accompanies regression output. It ranges in value from 0 to 1 and is usually interpreted as summarizing the percent of variation in the response that the regression model explains. So an R-squared of 0.65 might mean that the model explains about 65% of the variation in our dependent variable. Given this logic, we prefer our regression models have a high R-squared.\r\nIn R, we typically get R-squared by calling the summary function on a model object. Here’s a quick example using simulated data:\r\n\r\n\r\n# independent variable\r\nx <- 1:20 \r\n# for reproducibility\r\nset.seed(1) \r\n# dependent variable; function of x with random error\r\ny <- 2 + 0.5*x + rnorm(20,0,3) \r\n# simple linear regression\r\nmod <- lm(y~x)\r\n# request just the r-squared value\r\nsummary(mod)$r.squared          \r\n\r\n\r\n[1] 0.6026682\r\n\r\nOne way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations:\r\n\\[\r\nR^{2} =  \\frac{\\sum (\\hat{y} – \\bar{\\hat{y}})^{2}}{\\sum (y – \\bar{y})^{2}}\r\n\\]\r\nWe can calculate it directly using our model object like so:\r\n\r\n\r\n# extract fitted (or predicted) values from model\r\nf <- mod$fitted.values\r\n# sum of squared fitted-value deviations\r\nmss <- sum((f - mean(f))^2)\r\n# sum of squared original-value deviations\r\ntss <- sum((y - mean(y))^2)\r\n# r-squared\r\nmss/tss                      \r\n\r\n\r\n[1] 0.6026682\r\n\r\n1. R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making\\(σ^2\\) large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.\r\nWhat is \\(σ^2\\)? When we perform linear regression, we assume our model almost predicts our dependent variable. The difference between “almost” and “exact” is assumed to be a draw from a Normal distribution with mean 0 and some variance we call \\(σ^2\\).\r\nThis statement is easy enough to demonstrate. The way we do it here is to create a function that (1) generates data meeting the assumptions of simple linear regression (independent observations, normally distributed errors with constant variance), (2) fits a simple linear model to the data, and (3) reports the R-squared. Notice the only parameter for sake of simplicity is sigma. We then “apply” this function to a series of increasing \\(σ\\) values and plot the results.\r\n\r\n\r\nr2.0 <- function(sig){\r\n  # our predictor\r\n  x <- seq(1,10,length.out = 100)   \r\n  # our response; a function of x plus some random noise\r\n  y <- 2 + 1.2*x + rnorm(100,0,sd = sig) \r\n  # print the R-squared value\r\n  summary(lm(y ~ x))$r.squared          \r\n}\r\nsigmas <- seq(0.5,20,length.out = 20)\r\n # apply our function to a series of sigma values\r\nrout <- sapply(sigmas, r2.0)            \r\nplot(rout ~ sigmas, type=\"b\")\r\n\r\n\r\n\r\n\r\nR-squared tanks hard with increasing sigma, even though the model is completely correct in every respect.\r\nR-squared can be arbitrarily close to 1 when the model is totally wrong.\r\nThe point being made is that R-squared does not measure goodness of fit.\r\n\r\n\r\nset.seed(1)\r\n# our predictor is data from an exponential distribution\r\nx <- rexp(50,rate=0.005)\r\n# non-linear data generation\r\ny <- (x-1)^2 * runif(50, min=0.8, max=1.2) \r\n# clearly non-linear\r\nplot(x,y)             \r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.8485146\r\n\r\nIt’s very high at about 0.85, but the model is completely wrong. Using R-squared to justify the “goodness” of our model in this instance would be a mistake. Hopefully one would plot the data first and recognize that a simple linear regression in this case would be inappropriate.\r\n3. R-squared says nothing about prediction error, even with \\(σ^2\\) exactly the same, and no change in the coefficients. R-squared can be anywhere between 0 and 1 just by changing the range of X. We’re better off using Mean Square Error (MSE) as a measure of prediction error.\r\nMSE is basically the fitted y values minus the observed y values, squared, then summed, and then divided by the number of observations.\r\nLet’s demonstrate this statement by first generating data that meets all simple linear regression assumptions and then regressing y on x to assess both R-squared and MSE.\r\n\r\n\r\nx <- seq(1,10,length.out = 100)\r\nset.seed(1)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\r\nmod1 <- lm(y ~ x)\r\nsummary(mod1)$r.squared\r\n\r\n\r\n[1] 0.9383379\r\n\r\n# Mean squared error\r\nsum((fitted(mod1) - y)^2)/100\r\n\r\n\r\n[1] 0.6468052\r\n\r\nNow repeat the above code, but this time with a different range of x. Leave everything else the same:\r\n\r\n\r\n # new range of x\r\nx <- seq(1,2,length.out = 100)      \r\nset.seed(1)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\r\nmod1 <- lm(y ~ x)\r\nsummary(mod1)$r.squared\r\n\r\n\r\n[1] 0.1502448\r\n\r\n# Mean squared error\r\nsum((fitted(mod1) - y)^2)/100        \r\n\r\n\r\n[1] 0.6468052\r\n\r\nThe R-squared falls from 0.94 to 0.15 but the MSE remains the same. In other words the predictive ability is the same for both data sets, but the R-squared would lead you to believe the first example somehow had a model with more predictive power.\r\nR-squared can easily go down when the model assumptions are better fulfilled.\r\nLet’s examine this by generating data that would benefit from transformation. Notice the R code below is very much like our previous efforts but now we exponentiate our y variable.\r\n\r\n\r\nx <- seq(1,2,length.out = 100)\r\nset.seed(1)\r\ny <- exp(-2 - 0.09*x + rnorm(100,0,sd = 2.5))\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.003281718\r\n\r\nplot(lm(y ~ x), which=3)\r\n\r\n\r\n\r\n\r\nR-squared is very low and our residuals vs. fitted plot reveals outliers and non-constant variance. A common fix for this is to log transform the data. Let’s try that and see what happens:\r\n\r\n\r\nplot(lm(log(y)~x),which = 3) \r\n\r\n\r\n\r\n\r\nThe diagnostic plot looks much better. Our assumption of constant variance appears to be met. But look at the R-squared:\r\n\r\n\r\nsummary(lm(log(y)~x))$r.squared \r\n\r\n\r\n[1] 0.0006921086\r\n\r\nIt’s even lower! This is an extreme case and it doesn’t always happen like this. In fact, a log transformation will usually produce an increase in R-squared. But as just demonstrated, assumptions that are better fulfilled don’t always lead to higher R-squared.\r\nIt is very common to say that R-squared is “the fraction of variance explained” by the regression. \\[Yet\\] if we regressed X on Y, we’d get exactly the same R-squared. This in itself should be enough to show that a high R-squared says nothing about explaining one variable by another.\r\nThis is the easiest statement to demonstrate:\r\n\r\n\r\nx <- seq(1,10,length.out = 100)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 2)\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.737738\r\n\r\nsummary(lm(x ~ y))$r.squared\r\n\r\n\r\n[1] 0.737738\r\n\r\nDoes x explain y, or does y explain x? Are we saying “explain” to dance around the word “cause”? In a simple scenario with two variables such as this, R-squared is simply the square of the correlation between x and y:\r\n\r\n\r\nall.equal(cor(x,y)^2, summary(lm(x ~ y))$r.squared, summary(lm(y ~ x))$r.squared)\r\n\r\n\r\n[1] TRUE\r\n\r\nLet’s recap:\r\nR-squared does not measure goodness of fit.\r\nR-squared does not measure predictive error.\r\nR-squared does not necessarily increase when assumptions are better satisfied.\r\nR-squared does not measure how one variable explains another.\r\nAnother method to use is RMSE calculation.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-04T14:47:27-06:00"
    }
  ],
  "collections": []
}
